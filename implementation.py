# -*- coding: utf-8 -*-
"""Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ho0YwtfDhU2gaS5WXwB1Mc3_In2nWyC
"""

# Install missing packages
!pip install boruta
!pip install dask[dataframe]
!pip install catboost
!pip install lime


# Now import the necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.impute import KNNImputer
from boruta import BorutaPy
from xgboost import XGBRegressor, XGBClassifier
import lightgbm as lgb
import catboost as cb
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from lime.lime_tabular import LimeTabularExplainer
from sklearn.ensemble import StackingRegressor, StackingClassifier
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report

# Load Dataset
df = pd.read_csv('/content/test.csv')

# Data Cleaning and Preprocessing
# Separate numerical columns
numerical_cols = df.select_dtypes(include=[np.number]).columns

# Apply KNNImputer only to numerical columns
imputer = KNNImputer(n_neighbors=5)
df[numerical_cols] = imputer.fit_transform(df[numerical_cols])

# Remove Duplicates
df.drop_duplicates(subset=['Customer_ID'], inplace=True)

# Encode Categorical Features
label_encoders = {}
for col in ['Occupation', 'Payment_Behaviour', 'Credit_Mix']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

print(df.columns)

df.columns = df.columns.str.strip()

for col in df.columns:
    print(repr(col))

credit_limit = df.get('Credit_Limit')
print(credit_limit)

# Feature Engineering
# Ensure 'Credit_Limit' and other columns are correctly named

df['Debt_to_Income_Ratio'] = df['Total_EMI_per_month'] / df['Monthly_Inhand_Salary']
df['Credit_History_Length'] = df['Credit_History_Age'].str.extract('(\d+)').astype(float)

# Standardize the numerical features
scaler = StandardScaler()
numerical_cols = df.select_dtypes(include=[np.number]).columns  # Ensure you're scaling numerical columns
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

numeric_df = numeric_df.fillna(numeric_df.mean())

numeric_df = numeric_df.fillna(numeric_df.median())

numeric_df = numeric_df.fillna(0)

numeric_df = numeric_df.dropna()

numeric_df = numeric_df.dropna(axis=1)

print(numeric_df.isnull().sum())

# Clustering with DBSCAN, Hierarchical Clustering, and GMM
dbscan = DBSCAN(eps=0.5, min_samples=5).fit(numeric_df)
hierarchical = AgglomerativeClustering(n_clusters=6).fit(numeric_df)
gmm = GaussianMixture(n_components=6).fit_predict(numeric_df)

# Consensus Clustering (Voting Mechanism)
df['Cluster_Label'] = np.round((dbscan.labels_ + hierarchical.labels_ + gmm) / 3)

# Feature Selection and Weights for synthetic Credit score
weights = {
    'Credit_Utilization_Ratio': 0.25,
    'Debt_to_Income_Ratio': 0.20,
    'Credit_History_Age': 0.10,
    'Payment_Behaviour': 0.10,
    'Num_of_Delayed_Payment': 0.10,
    'Outstanding_Debt': 0.15,
    'Monthly_Balance': 0.10
}

# Standardize these features
features = [
    'Credit_Utilization_Ratio',
    'Debt_to_Income_Ratio',
    'Credit_History_Age',
    'Payment_Behaviour',
    'Num_of_Delayed_Payment',
    'Outstanding_Debt',
    'Monthly_Balance'
]

# Convert 'Credit_History_Age' to numerical value in months
# Ensure 'Credit_History_Age' is of string type before applying str.extract
df['Credit_History_Age'] = df['Credit_History_Age'].astype(str)  # Convert to string

# Extract years and convert to months
years = df['Credit_History_Age'].str.extract(r'(\d+)\s*Years?').astype(float).fillna(0) * 12

# Extract months
months = df['Credit_History_Age'].str.extract(r'(\d+)\s*Months?').astype(float).fillna(0)

# Combine years and months to get total months
df['Credit_History_Age'] = years + months

# Check the changes
print(df['Credit_History_Age'].head())

# Clean the features to ensure they are numeric
df[features] = df[features].replace(r'[^0-9.]', '', regex=True)  # Remove non-numeric characters
df[features] = df[features].apply(pd.to_numeric, errors='coerce')  # Convert to numeric, invalid parsing will be NaN

# Now apply scaling to the features
scaler = StandardScaler()
df[features] = scaler.fit_transform(df[features])

# Check the cleaned and scaled features
print(df[features].head())

# Calculate the weighted credit score
df['Credit_Score'] = (
    df['Credit_Utilization_Ratio'] * weights['Credit_Utilization_Ratio'] +
    df['Debt_to_Income_Ratio'] * weights['Debt_to_Income_Ratio'] +
    df['Credit_History_Age'] * weights['Credit_History_Age'] +
    df['Payment_Behaviour'] * weights['Payment_Behaviour'] +
    df['Num_of_Delayed_Payment'] * weights['Num_of_Delayed_Payment'] +
    df['Outstanding_Debt'] * weights['Outstanding_Debt'] +
    df['Monthly_Balance'] * weights['Monthly_Balance']
)

# Use the credit score as the target variable for prediction
X = df.drop(columns=['Credit_Score'])
y = df['Credit_Score']

print(df[['Customer_ID', 'Credit_Score']].head())

# Credit Score Prediction (Ensemble Regression)
X = df.drop(columns=['Credit_Score'])
y = df['Credit_Score']

from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# Identify non-numeric columns
non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns

# Handle non-numeric columns by encoding them
label_encoders = {}
for col in non_numeric_cols:
    le = LabelEncoder()
    # Impute missing values with a placeholder (e.g., 'missing') before encoding
    df[col] = df[col].fillna('missing').astype(str)
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Separate features and target
X = df.drop(columns=['Credit_Score'])
y = df['Credit_Score']

# Impute missing values in numeric features (X) before applying Boruta
imputer_X = SimpleImputer(strategy='mean')
X_imputed = imputer_X.fit_transform(X) # Assign imputed values to X_imputed

# Impute missing values in the target variable (y)
imputer_y = SimpleImputer(strategy='mean') # Or use 'median' if more appropriate
y = imputer_y.fit_transform(y.values.reshape(-1, 1)) # Reshape y to a 2D array

# Now apply Boruta feature selection
boruta = BorutaPy(XGBRegressor(), n_estimators='auto', verbose=2)
boruta.fit(X_imputed, y.ravel()) # Ravel y back to 1D for BorutaPy, use X_imputed

# Filter the selected features
X_filtered = X_imputed[:, boruta.support_]

# Get the column names of the selected features using the original DataFrame's columns
selected_feature_names = X.columns[boruta.support_]

# Convert X_filtered back to DataFrame with column names
X_filtered = pd.DataFrame(X_filtered, columns=selected_feature_names)
print(X_filtered.head())

xgb_reg = XGBRegressor(objective='reg:squarederror')
lgb_reg = lgb.LGBMRegressor()
cb_reg = cb.CatBoostRegressor(verbose=0)

!pip install --upgrade scikit-learn

!pip install --upgrade catboost

from sklearn.base import BaseEstimator, RegressorMixin

class CatBoostRegressorWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, **kwargs):
        self.model = cb.CatBoostRegressor(**kwargs)

    def fit(self, X, y):
        self.model.fit(X, y)
        return self

    def predict(self, X):
        return self.model.predict(X)

    def __sklearn_tags__(self):  # Added __sklearn_tags__ for compatibility
        return {
            'estimator_type': 'regressor',
            'requires_y': True
        }

ensemble_reg = StackingRegressor(estimators=[('xgb', xgb_reg), ('lgb', lgb_reg), ('cb', CatBoostRegressorWrapper(verbose=0))])

from sklearn.ensemble import StackingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

#ensemble model: StackingRegressor
base_learners = [
    ('rf', RandomForestRegressor(n_estimators=10)),
    ('lr', LinearRegression())
]

# Initialize the StackingRegressor
ensemble_reg = StackingRegressor(estimators=base_learners, final_estimator=LinearRegression())

# Split data for training (if not already split)
X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=42)

# Fit the model
ensemble_reg.fit(X_train, y_train)

# Make predictions
y_pred_reg = ensemble_reg.predict(X_test)

# Evaluate the model
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred_reg)
r2 = r2_score(y_test, y_pred_reg)

print(f'Mean Squared Error: {mse}')
print(f'R-squared: {r2}')

# Credit Score Segmentation (Ensemble Classification)
y_class = pd.cut(y, bins=[0, 580, 670, 740, 800, 850], labels=[0, 1, 2, 3, 4])

# Remove rows with NaN in y_class
X_filtered = X_filtered[y_class.notna()]
y_class = y_class[y_class.notna()]

# Impute NaN with the most frequent category
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent')
y_class = imputer.fit_transform(y_class.reshape(-1, 1))  # Reshape is needed for SimpleImputer
y_class = y_class.flatten()  # Flatten back to original shape

print(np.unique(y_class))

print(np.unique(y_class, return_counts=True))

y_class = pd.cut(y, bins=[0, 550, 650, 750, 850], labels=[0, 1, 2, 3])

print(np.min(y), np.max(y))  # Check the min and max of the original target variable

print(y_class.value_counts())

y_class = pd.cut(y, bins=[0, 550, 650, 750, 850], labels=[0, 1, 2, 3], right=False)

y_class = pd.cut(y, bins=4, labels=False)  # Automatically creates 4 bins

!pip install --upgrade scikit-learn
!pip install --upgrade catboost

from catboost import CatBoostClassifier
from sklearn.base import BaseEstimator, ClassifierMixin

class CatBoostClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
        # Initialize the CatBoostClassifier with the given keyword arguments
        self.model = CatBoostClassifier(**kwargs)

    def fit(self, X, y):
        # Fit the CatBoost model
        self.model.fit(X, y)
        return self

    def predict(self, X):
        # Return predictions
        return self.model.predict(X)

    def predict_proba(self, X):
        # Return class probabilities
        return self.model.predict_proba(X)

    def get_params(self, deep=True):
        # Return parameters of the CatBoost model
        return self.model.get_params()

y_class = pd.cut(y, bins=[0, 550, 650, 750, 850], labels=[0, 1, 2, 3], right=False)
if (y_class == 0).sum() < 2:
    y_class = y_class.replace(0, 1)  # Replace class 0 with class 1

!pip install --upgrade scikit-learn
!pip install --upgrade catboost
from sklearn.neural_network import MLPClassifier
from catboost import CatBoostClassifier
from sklearn.base import BaseEstimator, ClassifierMixin
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
import lightgbm as lgb
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import classification_report

class CatBoostClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
        self.model = CatBoostClassifier(**kwargs)

    def fit(self, X, y):
        self.model.fit(X, y)
        return self

    def predict(self, X):
        return self.model.predict(X)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

    def get_params(self, deep=True):
        return self.model.get_params()

# Convert y_class to Pandas Series if necessary
if isinstance(y_class, np.ndarray):
    y_class = pd.Series(y_class)

# Ensure y_class has the same index as X_filtered
# Resetting the index before slicing
y_class = y_class.reset_index(drop=True)
X_filtered = X_filtered.reset_index(drop=True)

# Ensure y_class has the same length as X_filtered
y_class = y_class.iloc[:len(X_filtered)]  # Slice y_class to match X_filtered length

# Adjusted binning
y_class = pd.qcut(y_class, q=4, labels=False, duplicates='drop')  # Create 4 bins based on quantiles

# Standardize X_filtered
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_filtered)

# Perform train-test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_class, test_size=0.2, random_state=42, stratify=y_class
)

from sklearn.ensemble import StackingClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize base classifiers
xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)
lgb_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)
cat_clf = CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, verbose=0, random_seed=42)

# Define the stacked ensemble with a Neural Network meta-model
stacked_clf = StackingClassifier(
    estimators=[('xgb', xgb_clf), ('lgb', lgb_clf), ('cat', cat_clf)],
    final_estimator=MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500),
    passthrough=True
)

!pip install --upgrade scikit-learn
!pip install --upgrade xgboost
!pip install --upgrade lightgbm
!pip install --upgrade catboost

from catboost import CatBoostClassifier
from sklearn.base import BaseEstimator, ClassifierMixin

class CatBoostClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
        self.model = CatBoostClassifier(**kwargs)

    def fit(self, X, y):
        self.model.fit(X, y, verbose=0)
        return self

    def predict(self, X):
        return self.model.predict(X)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

    def get_params(self, deep=True):
        return self.model.get_params(deep)

    def set_params(self, **params):
        self.model.set_params(**params)
        return self

# Train Neural Network Meta-Model
meta_model = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500)
meta_model.fit(stacked_train, y_train)

# Predict using Meta-Model
y_pred = meta_model.predict(stacked_test)
y_pred_proba = meta_model.predict_proba(stacked_test)

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
auc_roc = roc_auc_score(pd.get_dummies(y_test), y_pred_proba, multi_class="ovr")

print(f"Accuracy Score: {accuracy:.4f}")
print("Classification Report:\n", classification_rep)
print(f"AUC-ROC Score: {auc_roc:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.inspection import permutation_importance

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'], yticklabels=['Class 0', 'Class 1', 'Class 2', 'Class 3'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Permutation Importance (optional)
perm_importance = permutation_importance(meta_model, stacked_test, y_test, n_repeats=10, random_state=42)
feature_importances = perm_importance.importances_mean

# Visualize Feature Importances (for neural networks, you might be interested in examining the mean feature importances)
plt.figure(figsize=(8, 6))
plt.barh(range(len(feature_importances)), feature_importances)
plt.xlabel('Permutation Importance')
plt.ylabel('Features')
plt.title('Feature Importances')
plt.show()

# Cross-validation (optional)
from sklearn.model_selection import cross_val_score

cross_val_accuracy = cross_val_score(meta_model, stacked_test, y_test, cv=5, scoring='accuracy')
print(f"Cross-Validation Accuracy Scores: {cross_val_accuracy}")
print(f"Mean Cross-Validation Accuracy: {cross_val_accuracy.mean():.4f}")

import numpy as np

# Examine the unique values and their counts
unique_values, counts = np.unique(y_class, return_counts=True)
print(f"Unique values in y_class: {unique_values}")
print(f"Counts of unique values: {counts}")

# Use pd.qcut to create bins with quantiles
y_class = pd.qcut(y, q=4, labels=False, duplicates='drop')  # Create 4 bins based on quantiles

y_class = pd.cut(y, bins=[0, 550, 650, 750, 850], labels=[0, 1, 2, 3], right=False)

!pip install --upgrade scikit-learn
!pip install --upgrade xgboost
!pip install --upgrade lightgbm
!pip install --upgrade catboost

from sklearn.base import BaseEstimator, ClassifierMixin
from catboost import CatBoostClassifier

class CatBoostClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
        self.model = CatBoostClassifier(**kwargs)

    def fit(self, X, y):
        self.model.fit(X, y, verbose=0)  # Adjust verbose as needed
        return self

    def predict(self, X):
        return self.model.predict(X)

    def predict_proba(self, X):
        return self.model.predict_proba(X)

    def get_params(self, deep=True):
        # Ensure compatibility with get_params and set_params
        return self.model.get_params(deep)

    def set_params(self, **params):
        self.model.set_params(**params)
        return self

    def __sklearn_tags__(self):  # Add this method for compatibility
        return {
            'estimator_type': 'classifier',
            'requires_y': True
        }

ensemble_clf = StackingClassifier(
       estimators=[
           ('xgb', XGBClassifier()),
           ('lgb', lgb.LGBMClassifier()),
           ('cb', CatBoostClassifierWrapper())
       ],
       final_estimator=LogisticRegression()
   )

# Explainability with LIME
explainer = LimeTabularExplainer(X_filtered.values, feature_names=X_filtered.columns, mode='regression')
exp = explainer.explain_instance(X_filtered.iloc[0].values, ensemble_reg.predict)
exp.show_in_notebook()

import joblib

# Assuming 'ensemble_reg' and 'ensemble_clf' are your trained models
# Save the regression model
joblib.dump(ensemble_reg, 'ensemble_regression_model.pkl')

# Save the classification model
joblib.dump(ensemble_clf, 'ensemble_classification_model.pkl')

# To load the models later:
# loaded_reg_model = joblib.load('ensemble_regression_model.pkl')
# loaded_clf_model = joblib.load('ensemble_classification_model.pkl')

import joblib

# Load the saved models
loaded_reg_model = joblib.load('ensemble_regression_model.pkl')
loaded_clf_model = joblib.load('ensemble_classification_model.pkl')

# Now you can use loaded_reg_model and loaded_clf_model to make predictions
# Example:
# new_data = ... # Prepare your new data (make sure it has the same features used during training)
# predictions_reg = loaded_reg_model.predict(new_data)
# predictions_clf = loaded_clf_model.predict(new_data)